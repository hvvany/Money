#!/usr/bin/env python3
"""
ë¡œì»¬ ìš”ì•½ ìƒì„±ê¸°
OpenAI API ì—†ì´ë„ ì‘ë™í•˜ëŠ” ë¡œì»¬ ìš”ì•½ ì‹œìŠ¤í…œ
"""

import json
import os
import re
from datetime import datetime
from typing import List, Dict

class LocalSummarizer:
    def __init__(self):
        self.base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
    def load_news_data(self):
        """ë‰´ìŠ¤ ë°ì´í„° ë¡œë“œ"""
        news_path = os.path.join(self.base_dir, 'data', 'news.json')
        try:
            with open(news_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Error loading news data: {e}")
            return None
    
    def extract_keywords(self, text):
        """í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        economy_keywords = {
            'ì£¼ì‹ì‹œì¥': ['ì£¼ê°€', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì¦ì‹œ', 'ì£¼ì‹', 'íˆ¬ì', 'ì¦ê¶Œ', 'ETF', 'í€ë“œ'],
            'ë¶€ë™ì‚°': ['ë¶€ë™ì‚°', 'ì•„íŒŒíŠ¸', 'ì£¼íƒ', 'ë§¤ë§¤', 'ì„ëŒ€', 'ì „ì„¸', 'ì›”ì„¸', 'ì¬ê°œë°œ', 'ì¬ê±´ì¶•'],
            'ê¸ˆë¦¬ì •ì±…': ['ê¸ˆë¦¬', 'ì¤‘ì•™ì€í–‰', 'í•œêµ­ì€í–‰', 'ê¸°ì¤€ê¸ˆë¦¬', 'ì¸í”Œë ˆì´ì…˜', 'ë¬¼ê°€'],
            'ê²½ê¸°ë™í–¥': ['ê²½ê¸°', 'ì„±ì¥', 'GDP', 'ê²½ì œ', 'íšŒë³µ', 'ë¶€ì§„', 'í˜¸í™©', 'ì¹¨ì²´'],
            'êµ­ì œì •ì¹˜': ['APEC', 'ì •ìƒíšŒì˜', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ì™¸êµ', 'ë¬´ì—­'],
            'ê¸°ì—…': ['ì‚¼ì„±', 'LG', 'SK', 'í˜„ëŒ€', 'ê¸°ì—…', 'ë§¤ì¶œ', 'ìˆ˜ìµ', 'ì‹¤ì '],
            'ê³ ìš©': ['ê³ ìš©', 'ì·¨ì—…', 'ì‹¤ì—…', 'êµ¬ì§', 'ì±„ìš©', 'ë…¸ë™']
        }
        
        found_keywords = []
        text_lower = text.lower()
        
        for category, keywords in economy_keywords.items():
            for keyword in keywords:
                if keyword in text_lower:
                    found_keywords.append(category)
                    break
        
        return list(set(found_keywords))
    
    def generate_news_summary(self, news_item):
        """ê°œë³„ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±"""
        title = news_item.get('title', '')
        content = news_item.get('content', '')
        
        # ì œëª©ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ
        summary_parts = []
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ìš”ì•½
        keywords = self.extract_keywords(title + ' ' + content)
        
        if 'ì£¼ì‹ì‹œì¥' in keywords:
            if 'ì½”ìŠ¤í”¼' in title or 'ì¦ì‹œ' in title:
                summary_parts.append("ì£¼ì‹ì‹œì¥ì´ ìƒìŠ¹ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.")
            else:
                summary_parts.append("ì£¼ì‹ì‹œì¥ ê´€ë ¨ ë‰´ìŠ¤ê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.")
        
        if 'ë¶€ë™ì‚°' in keywords:
            if 'ì •ì±…' in title or 'ê³µê¸‰' in title:
                summary_parts.append("ë¶€ë™ì‚° ì •ì±… ê´€ë ¨ ë°œí‘œê°€ ìˆì—ˆìŠµë‹ˆë‹¤.")
            else:
                summary_parts.append("ë¶€ë™ì‚° ì‹œì¥ ë™í–¥ì´ ê´€ì‹¬ì„ ëŒê³  ìˆìŠµë‹ˆë‹¤.")
        
        if 'ê¸ˆë¦¬ì •ì±…' in keywords:
            summary_parts.append("ê¸ˆë¦¬ ì •ì±… ê´€ë ¨ ì†Œì‹ì´ ì „í•´ì¡ŒìŠµë‹ˆë‹¤.")
        
        if 'ê²½ê¸°ë™í–¥' in keywords:
            if 'íšŒë³µ' in title or 'ì„±ì¥' in title:
                summary_parts.append("ê²½ê¸° íšŒë³µ ì‹ í˜¸ê°€ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.")
            else:
                summary_parts.append("ê²½ì œ ë™í–¥ì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŠµë‹ˆë‹¤.")
        
        if 'êµ­ì œì •ì¹˜' in keywords:
            summary_parts.append("êµ­ì œ ì •ì¹˜ ê²½ì œ ì´ìŠˆê°€ ì£¼ëª©ë°›ê³  ìˆìŠµë‹ˆë‹¤.")
        
        # ê¸°ë³¸ ìš”ì•½ì´ ì—†ìœ¼ë©´ ì œëª© ê¸°ë°˜ìœ¼ë¡œ ìƒì„±
        if not summary_parts:
            if len(title) > 50:
                summary_parts.append(f"{title[:50]}...")
            else:
                summary_parts.append(title)
        
        return " ".join(summary_parts)
    
    def generate_daily_summary(self, news_list):
        """ì¼ì¼ ìš”ì•½ ìƒì„±"""
        if not news_list:
            return "ì˜¤ëŠ˜ì˜ ê²½ì œ ì´ìŠˆë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ì „ì²´ í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        for news in news_list:
            keywords = self.extract_keywords(news['title'] + ' ' + news['content'])
            all_keywords.extend(keywords)
        
        # í‚¤ì›Œë“œ ë¹ˆë„ ê³„ì‚°
        keyword_counts = {}
        for keyword in all_keywords:
            keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
        
        # ìƒìœ„ í‚¤ì›Œë“œ ì„ íƒ
        top_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)[:3]
        top_keyword_names = [kw[0] for kw in top_keywords]
        
        # ìš”ì•½ ìƒì„±
        summary_parts = []
        
        if top_keyword_names:
            summary_parts.append(f"ğŸ“ˆ **ì˜¤ëŠ˜ì˜ ì£¼ìš” ê²½ì œ ì´ìŠˆ**")
            summary_parts.append("")
            summary_parts.append(f"ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(top_keyword_names)}")
            summary_parts.append("")
        
        summary_parts.append("**ì£¼ìš” ë‰´ìŠ¤:**")
        
        for i, news in enumerate(news_list[:5], 1):
            title = news['title']
            if len(title) > 60:
                title = title[:60] + "..."
            summary_parts.append(f"{i}. {title}")
        
        return "\n".join(summary_parts)
    
    def process_news(self):
        """ë‰´ìŠ¤ ì²˜ë¦¬ ë° ìš”ì•½ ìƒì„±"""
        print("Loading news data...")
        data = self.load_news_data()
        
        if not data or 'news' not in data:
            print("No news data found.")
            return
        
        news_list = data['news']
        print(f"Processing {len(news_list)} news items...")
        
        # ê°œë³„ ë‰´ìŠ¤ ìš”ì•½ ìƒì„±
        for i, news in enumerate(news_list, 1):
            print(f"Summarizing news {i}/{len(news_list)}: {news['title'][:50]}...")
            summary = self.generate_news_summary(news)
            news['summary'] = summary
        
        # ì¼ì¼ ìš”ì•½ ìƒì„±
        print("Creating daily summary...")
        daily_summary = self.generate_daily_summary(news_list)
        
        # ë°ì´í„° ì—…ë°ì´íŠ¸
        data['summary'] = daily_summary
        data['lastUpdated'] = datetime.now().isoformat()
        
        # ì €ì¥
        output_path = os.path.join(self.base_dir, 'data', 'news.json')
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"Summarized data saved to {output_path}")
        print("Local summarization completed successfully!")

def main():
    summarizer = LocalSummarizer()
    summarizer.process_news()

if __name__ == "__main__":
    main()
