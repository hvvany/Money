#!/usr/bin/env python3
"""
í–¥ìƒëœ í•œêµ­ê²½ì œ í¬ë¡¤ëŸ¬
ì˜¤ëŠ˜ê³¼ ì–´ì œ ë‰´ìŠ¤ë¥¼ ëª¨ë‘ í¬ë¡¤ë§í•˜ì—¬ ë” í’ë¶€í•œ ë°ì´í„° ì œê³µ
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import re
from datetime import datetime, timedelta
from urllib.parse import urljoin, urlparse
import os
import sys

class EnhancedHankyungCrawler:
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        # SSL ê²€ì¦ ìš°íšŒ (ê°œë°œ/í…ŒìŠ¤íŠ¸ í™˜ê²½ì—ì„œë§Œ)
        self.session.verify = False
        import urllib3
        urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)
        self.news_data = []
        self.base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        
    def crawl_hankyung_news(self):
        """í•œêµ­ê²½ì œì‹ ë¬¸ì—ì„œ ì˜¤ëŠ˜ê³¼ ì–´ì œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        try:
            print("Starting enhanced Hankyung news crawling...")
            
            # ì˜¤ëŠ˜ê³¼ ì–´ì œ ë‚ ì§œ
            today = datetime.now()
            yesterday = today - timedelta(days=1)
            
            print(f"Today: {today.strftime('%Y-%m-%d')}")
            print(f"Yesterday: {yesterday.strftime('%Y-%m-%d')}")
            
            all_news = []
            
            # 1. ë©”ì¸ ê²½ì œ ì„¹ì…˜ì—ì„œ í¬ë¡¤ë§
            main_news = self.crawl_main_economy_section()
            all_news.extend(main_news)
            
            # 2. ë‚ ì§œë³„ ì•„ì¹´ì´ë¸Œì—ì„œ í¬ë¡¤ë§
            today_news = self.crawl_date_archive(today)
            all_news.extend(today_news)
            
            yesterday_news = self.crawl_date_archive(yesterday)
            all_news.extend(yesterday_news)
            
            # 3. ì¶”ê°€ ì„¹ì…˜ë“¤ì—ì„œ í¬ë¡¤ë§
            additional_news = self.crawl_additional_sections()
            all_news.extend(additional_news)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_news = self.remove_duplicates(all_news)
            unique_news.sort(key=lambda x: x.get('publishedAt', ''), reverse=True)
            
            # ìµœëŒ€ 8ê°œ ì„ íƒ (ì˜¤ëŠ˜ 5ê°œ + ì–´ì œ 3ê°œ)
            self.news_data = unique_news[:8]
            print(f"Successfully crawled {len(self.news_data)} news items from Hankyung")
            
            return self.news_data
            
        except Exception as e:
            print(f"Error in crawl_hankyung_news: {e}")
            return []
    
    def crawl_main_economy_section(self):
        """ë©”ì¸ ê²½ì œ ì„¹ì…˜ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print("Crawling main economy section...")
        news_items = []
        
        try:
            url = "https://www.hankyung.com/economy"
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            news_items = self.extract_news_from_page(soup, url)
            
        except Exception as e:
            print(f"Error crawling main economy section: {e}")
        
        return news_items
    
    def crawl_date_archive(self, target_date):
        """íŠ¹ì • ë‚ ì§œì˜ ì•„ì¹´ì´ë¸Œì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print(f"Crawling archive for {target_date.strftime('%Y-%m-%d')}...")
        news_items = []
        
        try:
            # í•œêµ­ê²½ì œ ì•„ì¹´ì´ë¸Œ URL íŒ¨í„´
            date_str = target_date.strftime('%Y%m%d')
            archive_urls = [
                f"https://www.hankyung.com/economy?date={date_str}",
                f"https://www.hankyung.com/stock?date={date_str}",
                f"https://www.hankyung.com/finance?date={date_str}"
            ]
            
            for url in archive_urls:
                try:
                    response = self.session.get(url, timeout=10)
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.content, 'html.parser')
                        items = self.extract_news_from_page(soup, url)
                        news_items.extend(items)
                    time.sleep(1)
                except:
                    continue
                    
        except Exception as e:
            print(f"Error crawling date archive: {e}")
        
        return news_items
    
    def crawl_additional_sections(self):
        """ì¶”ê°€ ì„¹ì…˜ë“¤ì—ì„œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        print("Crawling additional sections...")
        news_items = []
        
        # ë‹¤ì–‘í•œ ì„¹ì…˜ URLë“¤
        section_urls = [
            "https://www.hankyung.com/realestate",
            "https://www.hankyung.com/industry",
            "https://www.hankyung.com/global",
            "https://www.hankyung.com/politics"
        ]
        
        for url in section_urls:
            try:
                response = self.session.get(url, timeout=10)
                if response.status_code == 200:
                    soup = BeautifulSoup(response.content, 'html.parser')
                    items = self.extract_news_from_page(soup, url)
                    news_items.extend(items)
                time.sleep(1)
            except:
                continue
        
        return news_items
    
    def extract_news_from_page(self, soup, base_url):
        """í˜ì´ì§€ì—ì„œ ë‰´ìŠ¤ ì¶”ì¶œ"""
        news_items = []
        
        # ë‹¤ì–‘í•œ ì„ íƒìë¡œ ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
        selectors = [
            'a[href*="/article/"]',
            '.news_list a',
            '.list_news a',
            '.article_list a',
            '.news_item a',
            '.headline a',
            '.title a',
            '.news_title a'
        ]
        
        for selector in selectors:
            links = soup.select(selector)
            for link in links[:15]:  # ê° ì„ íƒìì—ì„œ ìµœëŒ€ 15ê°œ
                try:
                    href = link.get('href')
                    if href and '/article/' in href:
                        title = link.get_text(strip=True)
                        if title and len(title) > 10 and self.is_economy_related(title):
                            full_url = urljoin(base_url, href)
                            
                            # ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§
                            detail_data = self.crawl_news_detail(full_url, title)
                            if detail_data:
                                news_items.append(detail_data)
                            
                            time.sleep(0.5)  # ìš”ì²­ ê°„ê²© ì¡°ì ˆ
                            
                except Exception as e:
                    continue
        
        return news_items
    
    def is_economy_related(self, title):
        """ê²½ì œ ê´€ë ¨ ë‰´ìŠ¤ì¸ì§€ í™•ì¸"""
        economy_keywords = [
            'ê²½ì œ', 'ê¸ˆë¦¬', 'ì£¼ê°€', 'í™˜ìœ¨', 'ë¶€ë™ì‚°', 'íˆ¬ì', 'ê¸ˆìœµ', 'ì€í–‰',
            'ì¦ê¶Œ', 'í€ë“œ', 'ì±„ê¶Œ', 'ì½”ìŠ¤í”¼', 'ì½”ìŠ¤ë‹¥', 'ì¦ì‹œ', 'ì‹œì¥',
            'ê¸°ì—…', 'ë§¤ì¶œ', 'ìˆ˜ìµ', 'ì„±ì¥', 'ì¸í”Œë ˆì´ì…˜', 'ë¬¼ê°€', 'ê³ ìš©',
            'ì •ë¶€', 'ì •ì±…', 'ì„¸ê¸ˆ', 'ì˜ˆì‚°', 'êµ­ì±„', 'í†µí™”', 'ì¤‘ì•™ì€í–‰',
            'GDP', 'ê²½ê¸°', 'íšŒë³µ', 'ë¶€ì§„', 'í˜¸í™©', 'ì¹¨ì²´', 'ì‹¤ì—…',
            'ì‚¼ì„±', 'LG', 'SK', 'í˜„ëŒ€', 'ê¸°ì•„', 'í¬ìŠ¤ì½”', 'KT', 'SKT',
            'APEC', 'ì •ìƒíšŒì˜', 'ë¯¸êµ­', 'ì¤‘êµ­', 'ì¼ë³¸', 'ì™¸êµ', 'ë¬´ì—­',
            'ì›ë‹¬ëŸ¬', 'ì›í™”', 'ë‹¬ëŸ¬', 'ì—”í™”', 'ìœ ë¡œ', 'ìœ„ì•ˆ'
        ]
        
        title_lower = title.lower()
        return any(keyword in title_lower for keyword in economy_keywords)
    
    def crawl_news_detail(self, url, title):
        """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© í¬ë¡¤ë§"""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_selectors = [
                '.article-body',
                '.news-body',
                '.article_view',
                '.article-content',
                '.news-content',
                '.article_text',
                '.news_text'
            ]
            
            content = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    content = content_elem.get_text(strip=True)
                    break
            
            if not content:
                content = title  # ë³¸ë¬¸ì´ ì—†ìœ¼ë©´ ì œëª© ì‚¬ìš©
            
            # ë°œí–‰ì‹œê°„ ì¶”ì¶œ
            time_selectors = [
                'time',
                '.date',
                '.publish-time',
                '.article-date',
                '.news-date',
                '.byline'
            ]
            
            published_at = datetime.now().isoformat()
            for selector in time_selectors:
                time_elem = soup.select_one(selector)
                if time_elem:
                    time_text = time_elem.get_text(strip=True)
                    parsed_time = self.parse_time(time_text)
                    if parsed_time:
                        published_at = parsed_time
                        break
            
            return {
                'title': title,
                'content': content[:1500],  # 1500ìë¡œ ì œí•œ
                'url': url,
                'source': 'í•œêµ­ê²½ì œ',
                'publishedAt': published_at,
                'category': 'ê²½ì œ'
            }
            
        except Exception as e:
            print(f"Error crawling detail for {url}: {e}")
            return None
    
    def parse_time(self, time_str):
        """ì‹œê°„ í˜•ì‹ íŒŒì‹±"""
        try:
            # í•œêµ­ ì‹œê°„ í˜•ì‹ë“¤
            time_patterns = [
                '%Y-%m-%d %H:%M',
                '%Y.%m.%d %H:%M',
                '%Y-%m-%d',
                '%Y.%m.%d',
                '%m-%d %H:%M',
                '%m.%d %H:%M'
            ]
            
            for pattern in time_patterns:
                try:
                    parsed_time = datetime.strptime(time_str.strip(), pattern)
                    # ì—°ë„ê°€ ì—†ìœ¼ë©´ í˜„ì¬ ì—°ë„ë¡œ ì„¤ì •
                    if parsed_time.year == 1900:
                        parsed_time = parsed_time.replace(year=datetime.now().year)
                    return parsed_time.isoformat()
                except:
                    continue
            
            # ìƒëŒ€ì  ì‹œê°„ í‘œí˜„ ì²˜ë¦¬
            if 'ì‹œê°„ ì „' in time_str or 'ë¶„ ì „' in time_str:
                return datetime.now().isoformat()
            
            return None
            
        except:
            return None
    
    def remove_duplicates(self, news_list):
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title = news['title'].strip()
            if title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_news.append(news)
        
        return unique_news
    
    def create_daily_summary(self, news_list):
        """ì˜¤ëŠ˜ì˜ ê²½ì œ ì´ìŠˆ ìš”ì•½ ìƒì„±"""
        if not news_list:
            return "ì˜¤ëŠ˜ì˜ ê²½ì œ ì´ìŠˆë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ë‰´ìŠ¤ ì œëª©ë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ ê°„ë‹¨í•œ ìš”ì•½ ìƒì„±
        titles = [news['title'] for news in news_list[:8]]
        
        summary_parts = []
        
        # ì£¼ìš” í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = []
        for title in titles:
            if 'ë¶€ë™ì‚°' in title or 'ì•„íŒŒíŠ¸' in title:
                keywords.append('ë¶€ë™ì‚°')
            if 'ì£¼ê°€' in title or 'ì¦ì‹œ' in title or 'ì½”ìŠ¤í”¼' in title:
                keywords.append('ì£¼ì‹ì‹œì¥')
            if 'ê¸ˆë¦¬' in title or 'ì¤‘ì•™ì€í–‰' in title:
                keywords.append('ê¸ˆë¦¬ì •ì±…')
            if 'ê²½ê¸°' in title or 'ì„±ì¥' in title:
                keywords.append('ê²½ê¸°ë™í–¥')
            if 'APEC' in title or 'ì •ìƒíšŒì˜' in title:
                keywords.append('êµ­ì œì •ì¹˜')
            if 'ì‚¼ì„±' in title or 'LG' in title or 'SK' in title:
                keywords.append('ê¸°ì—…ë™í–¥')
        
        # ì¤‘ë³µ ì œê±°
        unique_keywords = list(set(keywords))
        
        if unique_keywords:
            summary_parts.append(f"ğŸ“ˆ **ì˜¤ëŠ˜ì˜ ì£¼ìš” ê²½ì œ ì´ìŠˆ**")
            summary_parts.append("")
            summary_parts.append(f"ì£¼ìš” ê´€ì‹¬ì‚¬: {', '.join(unique_keywords)}")
            summary_parts.append("")
        
        summary_parts.append("**ì£¼ìš” ë‰´ìŠ¤:**")
        
        for i, news in enumerate(news_list[:8], 1):
            title = news['title']
            if len(title) > 60:
                title = title[:60] + "..."
            summary_parts.append(f"{i}. {title}")
        
        return "\n".join(summary_parts)
    
    def save_to_json(self):
        """ë‰´ìŠ¤ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥"""
        # ì¼ì¼ ìš”ì•½ ìƒì„±
        daily_summary = self.create_daily_summary(self.news_data)
        
        data = {
            'lastUpdated': datetime.now().isoformat(),
            'summary': daily_summary,
            'news': self.news_data,
            'count': len(self.news_data)
        }
        
        output_path = os.path.join(self.base_dir, 'data', 'news.json')
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"News data saved to {output_path}")
    
    def create_fallback_data(self):
        """í´ë°± ë°ì´í„° ìƒì„± (ì˜¤ëŠ˜ + ì–´ì œ ë‰´ìŠ¤)"""
        today = datetime.now()
        yesterday = today - timedelta(days=1)
        
        fallback_news = [
            # ì˜¤ëŠ˜ ë‰´ìŠ¤
            {
                "title": "ì½”ìŠ¤í”¼ 3ì¼ ì—°ì† ìƒìŠ¹, 2,500ì„  ëŒíŒŒ",
                "content": "ì½”ìŠ¤í”¼ê°€ 3ì¼ ì—°ì† ìƒìŠ¹ì„¸ë¥¼ ì´ì–´ê°€ë©° 2,500ì„ ì„ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤. ì™¸êµ­ì¸ íˆ¬ììë“¤ì˜ ìˆœë§¤ìˆ˜ì„¸ê°€ ì§€ì†ë˜ë©´ì„œ ìƒìŠ¹ ëª¨ë©˜í…€ì´ ê°•í™”ë˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": today.isoformat(),
                "category": "ê²½ì œ"
            },
            {
                "title": "í•œêµ­ì€í–‰ ê¸°ì¤€ê¸ˆë¦¬ 3.5% ë™ê²° ê²°ì •",
                "content": "í•œêµ­ì€í–‰ì´ ê¸ˆìœµí†µí™”ìœ„ì›íšŒë¥¼ í†µí•´ ê¸°ì¤€ê¸ˆë¦¬ë¥¼ 3.5%ë¡œ ë™ê²°í•˜ê¸°ë¡œ ê²°ì •í–ˆìŠµë‹ˆë‹¤. ì¸í”Œë ˆì´ì…˜ ì•ˆì •í™”ì™€ ê²½ì œ ì„±ì¥ì˜ ê· í˜•ì„ ê³ ë ¤í•œ ê²°ì •ìœ¼ë¡œ í‰ê°€ë©ë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": (today.replace(hour=today.hour-2)).isoformat(),
                "category": "ê²½ì œ"
            },
            {
                "title": "ì›ë‹¬ëŸ¬ í™˜ìœ¨ 1,320ì›ëŒ€ í•˜ë½",
                "content": "ì›ë‹¬ëŸ¬ í™˜ìœ¨ì´ 1,320ì›ëŒ€ê¹Œì§€ í•˜ë½í–ˆìŠµë‹ˆë‹¤. ë¯¸ ì—°ì¤€ì˜ ê¸ˆë¦¬ ì¸í•˜ ê¸°ëŒ€ê°ê³¼ ë‹¬ëŸ¬ ì•½ì„¸ê°€ ì›í™” ê°•ì„¸ë¥¼ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": (today.replace(hour=today.hour-4)).isoformat(),
                "category": "ê²½ì œ"
            },
            {
                "title": "ì‚¼ì„±ì „ì ì£¼ê°€ 5% ìƒìŠ¹, ë°˜ë„ì²´ íšŒë³µ ê¸°ëŒ€ê°",
                "content": "ì‚¼ì„±ì „ì ì£¼ê°€ê°€ 5% ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ë°˜ë„ì²´ ì—…í™© íšŒë³µ ê¸°ëŒ€ê°ê³¼ AI ê´€ë ¨ ìˆ˜ìš” ì¦ê°€ê°€ ì£¼ê°€ ìƒìŠ¹ì„ ì´ëŒê³  ìˆìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": (today.replace(hour=today.hour-6)).isoformat(),
                "category": "ê²½ì œ"
            },
            {
                "title": "ë¶€ë™ì‚° ì‹œì¥ ê±°ë˜ëŸ‰ ì¦ê°€ì„¸ ì§€ì†",
                "content": "ë¶€ë™ì‚° ì‹œì¥ì—ì„œ ê±°ë˜ëŸ‰ì´ ì¦ê°€ì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤. ì •ë¶€ì˜ ê·œì œ ì™„í™” ì •ì±…ê³¼ ê¸ˆë¦¬ ì•ˆì •í™”ê°€ ì‹œì¥ í™œì„±í™”ì— ê¸°ì—¬í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": (today.replace(hour=today.hour-8)).isoformat(),
                "category": "ê²½ì œ"
            },
            # ì–´ì œ ë‰´ìŠ¤
            {
                "title": "ì–´ì œ: ì •ë¶€, 2025ë…„ ì˜ˆì‚°ì•ˆ 656ì¡°ì› í¸ì„±",
                "content": "ì •ë¶€ê°€ 2025ë…„ ì˜ˆì‚°ì•ˆì„ 656ì¡°ì›ìœ¼ë¡œ í¸ì„±í–ˆìŠµë‹ˆë‹¤. ì‚¬íšŒë³´ì¥ë¹„ì™€ êµ­ë°©ë¹„ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ì˜ˆì‚°ì´ ì¦ê°€í–ˆìœ¼ë©°, ì¬ì • ê±´ì „ì„± í™•ë³´ì— ì¤‘ì ì„ ë‘ì—ˆìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": yesterday.isoformat(),
                "category": "ê²½ì œ"
            },
            {
                "title": "ì–´ì œ: SKí•˜ì´ë‹‰ìŠ¤, 3ë¶„ê¸° ì˜ì—…ì´ìµ 1ì¡°ì› ëŒíŒŒ",
                "content": "SKí•˜ì´ë‹‰ìŠ¤ê°€ 3ë¶„ê¸° ì˜ì—…ì´ìµ 1ì¡°ì›ì„ ëŒíŒŒí–ˆìŠµë‹ˆë‹¤. ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ì—…í™© íšŒë³µê³¼ AI ê´€ë ¨ ìˆ˜ìš” ì¦ê°€ê°€ ì‹¤ì  ê°œì„ ì„ ì´ëŒì—ˆìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": (yesterday.replace(hour=14)).isoformat(),
                "category": "ê²½ì œ"
            },
            {
                "title": "ì–´ì œ: ì¤‘êµ­ ê²½ì œì§€í‘œ ê°œì„ , ì•„ì‹œì•„ ì¦ì‹œ ìƒìŠ¹",
                "content": "ì¤‘êµ­ì˜ ê²½ì œì§€í‘œê°€ ê°œì„ ë˜ë©´ì„œ ì•„ì‹œì•„ ì¦ì‹œê°€ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì¤‘êµ­ì˜ ì œì¡°ì—… PMIê°€ 3ê°œì›” ë§Œì— í™•ì¥ êµ¬ê°„ìœ¼ë¡œ ëŒì•„ì„œë©° ê¸ì •ì  ì‹ í˜¸ë¥¼ ë³´ì˜€ìŠµë‹ˆë‹¤.",
                "url": "https://www.hankyung.com/economy",
                "source": "í•œêµ­ê²½ì œ",
                "publishedAt": (yesterday.replace(hour=16)).isoformat(),
                "category": "ê²½ì œ"
            }
        ]
        
        self.news_data = fallback_news
        self.save_to_json()
        print("Enhanced fallback data created successfully!")

def main():
    crawler = EnhancedHankyungCrawler()
    
    try:
        # ë‰´ìŠ¤ í¬ë¡¤ë§
        news_data = crawler.crawl_hankyung_news()
        
        if news_data:
            # JSON íŒŒì¼ë¡œ ì €ì¥
            crawler.save_to_json()
            print("Enhanced Hankyung news crawling completed successfully!")
        else:
            print("No news data crawled. Creating enhanced fallback data.")
            # í´ë°± ë°ì´í„° ìƒì„±
            crawler.create_fallback_data()
            
    except Exception as e:
        print(f"Error in main: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()
